{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-Base CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fcolombo7/AN2DL-2020/blob/main/Final%20Notebooks/2_Base_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyGfDyE8MEyZ"
      },
      "source": [
        "# **AN2DL** - Image classification challenge\n",
        "* **Colombo** Filippo - 10559531\n",
        "* **Del Vecchio** Giovanni - 10570682"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URPWyV4iMQYv"
      },
      "source": [
        "## CNN model (without transfer learning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCwx6rVCMdGI"
      },
      "source": [
        "Configuration and constants:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Mzov4HfF2U"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-4aT9zgG0oe"
      },
      "source": [
        "#Random seed to make experiments reproducible\n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)  \n",
        "\n",
        "#Parameters\n",
        "IMG_H, IMG_W = (256, 256)\n",
        "BS = 64 #BATCH SIZE\n",
        "VALIDATION_SPLIT = 0.2\n",
        "DATA_AUGMENTATION = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUOZsQODNCK4"
      },
      "source": [
        "Load Google Drive to get the data and save the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZp3vyhvG8KO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f3bb38-0c21-41c4-ce52-e303f67a47b4"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "cwd = os.getcwd()\n",
        "drive_root_folder = '/content/drive/My Drive/ANN_project/'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_50jGtxNa-0"
      },
      "source": [
        "### Import the Mask Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipFiRLDRKh1e"
      },
      "source": [
        "Check if the dataset has been already processed, otherwise unzip it.\n",
        "\n",
        "**N.B.**: we are unzipping in the `/content` folder and not in `drive`.  \n",
        "If you want to unzip in your drive folder, write `os.chdir(drive_root_folder)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHDb6talfR-t",
        "outputId": "20d3b53f-89ca-4ea9-bfd3-a455c8316555",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#check if the dataset is already available\n",
        "if not os.path.exists(cwd+'/MaskDataset'):\n",
        "  !unzip '/content/drive/My Drive/ANN_project/artificial-neural-networks-and-deep-learning-2020.zip'\n",
        "else:\n",
        "  print('MaskDataset already loaded')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MaskDataset already loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUNMDIj2O3kw"
      },
      "source": [
        "### Split the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlzyOqrMRfH0"
      },
      "source": [
        "Definition of the `ImageDataGenerator` for _data augmentation_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODKLCXMJhFnu"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "if DATA_AUGMENTATION:\n",
        "    train_data_gen = ImageDataGenerator(rotation_range=20, \n",
        "                                        width_shift_range=0.3, \n",
        "                                        height_shift_range=0.3, \n",
        "                                        zoom_range=0.4, \n",
        "                                        horizontal_flip=True, #\n",
        "                                        shear_range=10, \n",
        "                                        channel_shift_range=100, \n",
        "                                        fill_mode='reflect', \n",
        "                                        rescale=1./255)\n",
        "else:\n",
        "    train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "valid_data_gen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YQh2TsygS-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5407fc37-2c40-4eba-8107-a35823ba450c"
      },
      "source": [
        "import json\n",
        "\n",
        "dataset_dir = os.path.join(cwd, \"MaskDataset\")\n",
        "training_dir = os.path.join(dataset_dir, \"training\")\n",
        "with open(os.path.join(dataset_dir,\"train_gt.json\")) as f:\n",
        "  dic = json.load(f)\n",
        "  dataframe = pd.DataFrame(dic.items())\n",
        "  dataframe.rename(columns = {0:'filename', 1:'class'}, inplace = True)\n",
        "  dataframe = dataframe.sample(frac=1, random_state=SEED)\n",
        "  \n",
        "  tot_length = dataframe.shape[0]\n",
        "  valid = dataframe.iloc[:int(np.ceil(tot_length * VALIDATION_SPLIT)),:] \n",
        "  train = dataframe.iloc[int(np.ceil(tot_length * VALIDATION_SPLIT)):,:]\n",
        "  train[\"class\"] = train[\"class\"].astype('string')\n",
        "  valid[\"class\"] = valid[\"class\"].astype('string')\n",
        "  \n",
        "  train_gen = train_data_gen.flow_from_dataframe(train,\n",
        "                                               training_dir,\n",
        "                                               batch_size=BS,\n",
        "                                               target_size=(IMG_H, IMG_W),\n",
        "                                               class_mode='categorical',\n",
        "                                               shuffle=True,\n",
        "                                               seed=SEED)\n",
        "  \n",
        "  validation_gen = valid_data_gen.flow_from_dataframe(valid,\n",
        "                                               training_dir,\n",
        "                                               batch_size=BS,\n",
        "                                               target_size=(IMG_H, IMG_W),\n",
        "                                               class_mode='categorical',\n",
        "                                               shuffle=True,\n",
        "                                               seed=SEED)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4491 validated image filenames belonging to 3 classes.\n",
            "Found 1123 validated image filenames belonging to 3 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bgWDFVZeeNx"
      },
      "source": [
        "### Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3JF94cte4zX"
      },
      "source": [
        "#### Create the dataset objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jwtQWEn1SXz"
      },
      "source": [
        "num_classes = len(train_gen.class_indices)\n",
        "num_classes\n",
        "train_ds = tf.data.Dataset.from_generator(lambda: train_gen, \n",
        "                                          output_types=(tf.float32, tf.float32),\n",
        "                                          output_shapes= ([None, IMG_H, IMG_W, 3], [None, num_classes]))\n",
        "\n",
        "train_ds = train_ds.repeat()\n",
        "\n",
        "valid_ds = tf.data.Dataset.from_generator(lambda: validation_gen,\n",
        "                                          output_types = (tf.float32, tf.float32),\n",
        "                                          output_shapes = ([None, IMG_H, IMG_W, 3], [None, num_classes]))\n",
        "valid_ds = valid_ds.repeat()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GxO1cGCQ3xE"
      },
      "source": [
        "#### CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGGAzFCJGcLd"
      },
      "source": [
        "def get_feature_extraction(start_f=8, depth=6, conv_per_depth = 1, activation='relu', kernel_size = 3, stride = (1, 1)):\n",
        "  # Architecture: Features extraction -> Classifier\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # Features extraction\n",
        "  for i in range(depth):\n",
        "\n",
        "      if i == 0:\n",
        "          input_shape = [IMG_H, IMG_W, 3]\n",
        "      else:\n",
        "          input_shape=[None]\n",
        "\n",
        "      # Conv block: Conv2D -> Activation -> Pooling\n",
        "      for n in range(conv_per_depth):\n",
        "        model.add(tf.keras.layers.Conv2D(filters=start_f, \n",
        "                                        kernel_size=(kernel_size, kernel_size),\n",
        "                                        strides=stride,\n",
        "                                        padding='same',\n",
        "                                        input_shape=input_shape,\n",
        "                                        activation = activation))\n",
        "      model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "      start_f *= 2\n",
        "\n",
        "  return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFdiypb8Gm8S"
      },
      "source": [
        "def get_classifier(model, hidden_layer=3, units=512, dropout=0.2, regularization=0.001, activation='relu'):\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  if dropout and regularization:\n",
        "    for i in range(hidden_layer):\n",
        "      model.add(tf.keras.layers.Dense(units=units, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization)))\n",
        "      model.add(tf.keras.layers.Dropout(dropout))\n",
        "  elif dropout:\n",
        "    for i in range(hidden_layer):\n",
        "      model.add(tf.keras.layers.Dense(units=units, activation=activation))\n",
        "      model.add(tf.keras.layers.Dropout(dropout))\n",
        "  elif regularization:\n",
        "    for i in range(hidden_layer):\n",
        "      model.add(tf.keras.layers.Dense(units=units, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization)))\n",
        "  else:\n",
        "    for i in range(hidden_layer):\n",
        "      model.add(tf.keras.layers.Dense(units=units, activation=activation))\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQXaGOBmGnuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d361d6b3-c56e-4ec2-8dbd-c16eff1aaba0"
      },
      "source": [
        "model = get_feature_extraction(depth = 5)\n",
        "model = get_classifier(model, hidden_layer=1, units=512, regularization=0.001, dropout=0.2)\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 256, 256, 8)       224       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 128, 128, 8)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 128, 128, 16)      1168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 64, 64, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               4194816   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 4,294,739\n",
            "Trainable params: 4,294,739\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URZq-McI5oqC"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKAwwLIgmxQ4"
      },
      "source": [
        "def get_optimizer(lr_schedule = 1e-3):\n",
        "  return tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HCbyUgxg_uC"
      },
      "source": [
        "def get_callbacks(name, es=True, patience = 10):\n",
        "  obj = []\n",
        "\n",
        "  exps_dir = os.path.join(drive_root_folder,'classification_result')\n",
        "  if not os.path.exists(exps_dir):\n",
        "    os.mkdir(exps_dir)\n",
        "\n",
        "  now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "  temp_dir = name +'_'+ now\n",
        "\n",
        "  #Model Checkpoints\n",
        "  ckpt_dir = os.path.join(exps_dir, temp_dir, 'ckpts')\n",
        "  if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        "\n",
        "  obj.append(tf.keras.callbacks.ModelCheckpoint(filepath = os.path.join(ckpt_dir, 'cp.ckpt'),\n",
        "                                                save_weights_only = True,\n",
        "                                                save_best_only = True))\n",
        "  \n",
        "  #Tensor board logs\n",
        "  tb_dir = os.path.join(exps_dir, temp_dir + '/tb_logs')\n",
        "  if not os.path.exists(tb_dir):\n",
        "    os.makedirs(tb_dir)\n",
        "  obj.append(tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
        "                                            profile_batch = 0,\n",
        "                                            histogram_freq = 1))\n",
        "  if es:\n",
        "    obj.append(tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = patience))\n",
        "\n",
        "  return obj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1H4ehVFF7PP"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7q2E_I86wWK"
      },
      "source": [
        "name = 'CNN-depth5' #check the model name in the drive folder (the time stamp changes)\n",
        "model.compile(optimizer=get_optimizer(),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x=train_ds,\n",
        "          epochs=100,\n",
        "          steps_per_epoch=len(train_gen),\n",
        "          validation_data=valid_ds,\n",
        "          validation_steps=len(validation_gen), \n",
        "          callbacks=get_callbacks(name, es=True, ),\n",
        "          verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7XT_pJ0V1ZT"
      },
      "source": [
        "### Load the best model according to the checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwcdESkdXNFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4481820-60b1-4f26-c1b8-de0e588b1a2d"
      },
      "source": [
        "model_to_load = 'CNN-depth5' #check the model name in the drive folder (the time stamp changes)\n",
        "ckpt_path = os.path.join(drive_root_folder,'classification_result',model_to_load, 'ckpts','cp.ckpt')\n",
        "model.load_weights(ckpt_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f49301b9dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi5B6RK5b52M"
      },
      "source": [
        "## Predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWNWkCg5cjsh"
      },
      "source": [
        "First load the test images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPFPEMYHeZCI"
      },
      "source": [
        "import pathlib\n",
        "path = os.path.join(os.getcwd(),'MaskDataset', 'test')\n",
        "data_dir = pathlib.Path(path)\n",
        "\n",
        "test_image_filenames = list(data_dir.glob('*.jpg'))\n",
        "len(test_image_filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26uSROvwXlKR"
      },
      "source": [
        "test_normalization = tf.keras.Sequential([\n",
        "                                       tf.keras.layers.experimental.preprocessing.Rescaling(1./255)                 \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1MZJxvKlG9D"
      },
      "source": [
        "def make_prediction():\n",
        "  results = {}\n",
        "  results_expanded = {}\n",
        "  for test_image in test_image_filenames:\n",
        "    img = tf.keras.preprocessing.image.load_img(test_image, target_size=(IMG_H, IMG_W))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "    normalized_img = test_normalization(img_array, training=True)\n",
        "    predictions = model.predict(normalized_img)\n",
        "    results_expanded[os.path.basename(test_image)]=predictions\n",
        "    results[os.path.basename(test_image)]=np.argmax(predictions)\n",
        "  return results, results_expanded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYXxxXNOkHdO"
      },
      "source": [
        "def create_csv(results, results_dir):\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "        f.write('Id,Category\\n')\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8_2J8skkajW"
      },
      "source": [
        "create_csv(make_prediction()[0], drive_root_folder)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}