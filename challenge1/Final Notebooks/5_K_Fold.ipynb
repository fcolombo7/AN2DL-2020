{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_K-Fold.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fcolombo7/AN2DL-2020/blob/main/Final%20Notebooks/5_K_Fold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8WEz1rYQBKa"
      },
      "source": [
        "# **AN2DL** - Image classification challenge\n",
        "\n",
        "* **Colombo** Filippo - 10559531\n",
        "* **Del Vecchio** Giovanni - 10570682\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URPWyV4iMQYv"
      },
      "source": [
        "## K-Fold cross validation \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCwx6rVCMdGI"
      },
      "source": [
        "Configuration and constants:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Mzov4HfF2U"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-4aT9zgG0oe"
      },
      "source": [
        "#Random seed to make experiments reproducible\n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)  \n",
        "\n",
        "#Parameters\n",
        "IMG_H, IMG_W = (448, 448)\n",
        "BS = 32 #BATCH SIZE\n",
        "VALIDATION_SPLIT = 0.2\n",
        "DATA_AUGMENTATION = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUOZsQODNCK4"
      },
      "source": [
        "Load Google Drive to get the data and save the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZp3vyhvG8KO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd4eb04-769d-442c-f1f8-337756d8300b"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "cwd = os.getcwd()\n",
        "drive_root_folder = '/content/drive/My Drive/ANN_project/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_50jGtxNa-0"
      },
      "source": [
        "### Import the Mask Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipFiRLDRKh1e"
      },
      "source": [
        "Check if the dataset has been already processed, otherwise unzip it.\n",
        "\n",
        "**N.B.**: we are unzipping in the `/content` folder and not in `drive`.  \n",
        "If you want to unzip in your drive folder, write `os.chdir(drive_root_folder)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHDb6talfR-t"
      },
      "source": [
        "#check if the dataset is already available\n",
        "if not os.path.exists(cwd+'/MaskDataset'):\n",
        "  !unzip '/content/drive/My Drive/ANN_project/artificial-neural-networks-and-deep-learning-2020.zip'\n",
        "else:\n",
        "  print('MaskDataset already loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUNMDIj2O3kw"
      },
      "source": [
        "#### Define the generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlzyOqrMRfH0"
      },
      "source": [
        "Definition of the `ImageDataGenerator` for _data augmentation_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODKLCXMJhFnu"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "if DATA_AUGMENTATION:\n",
        "    train_data_gen = ImageDataGenerator(rotation_range=20, \n",
        "                                        width_shift_range=0.3, \n",
        "                                        height_shift_range=0.3, \n",
        "                                        zoom_range=0.4, \n",
        "                                        horizontal_flip=True, \n",
        "                                        shear_range=10, \n",
        "                                        channel_shift_range=100, \n",
        "                                        fill_mode='reflect', \n",
        "                                        rescale=1./255)\n",
        "else:\n",
        "    train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "valid_data_gen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-LvRAbxXnZJ"
      },
      "source": [
        "### K-Fold procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvGg8qv9QgqL"
      },
      "source": [
        "def get_optimizer(lr_schedule = 1e-3):\n",
        "  return tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkYdUChUQkDP"
      },
      "source": [
        "def get_callbacks(name, es=True, LR = False , patience = 10):\n",
        "  obj = []\n",
        "\n",
        "  exps_dir = os.path.join(drive_root_folder,'classification_result')\n",
        "  if not os.path.exists(exps_dir):\n",
        "    os.mkdir(exps_dir)\n",
        "\n",
        "  now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "  temp_dir = name\n",
        "\n",
        "  #Model Checkpoints\n",
        "  ckpt_dir = os.path.join(exps_dir, temp_dir, 'ckpts')\n",
        "  if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        "\n",
        "  obj.append(tf.keras.callbacks.ModelCheckpoint(filepath = os.path.join(ckpt_dir, 'cp.ckpt'),\n",
        "                                                save_weights_only = True,\n",
        "                                                save_best_only = True))\n",
        "  \n",
        "  #Tensor board logs\n",
        "  tb_dir = os.path.join(exps_dir, temp_dir + '/tb_logs')\n",
        "  if not os.path.exists(tb_dir):\n",
        "    os.makedirs(tb_dir)\n",
        "  obj.append(tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
        "                                            profile_batch = 0,\n",
        "                                            histogram_freq = 1))\n",
        "  if es:\n",
        "    obj.append(tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = patience))\n",
        "\n",
        "  if LR:\n",
        "    obj.append(tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    mode=\"auto\",\n",
        "    min_lr=0.0001\n",
        "    ))\n",
        "\n",
        "  return obj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bPLEpx3w5pD"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "np.random.seed(SEED)\n",
        "\n",
        "import json\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset_dir = os.path.join(cwd, \"MaskDataset\")\n",
        "training_dir = os.path.join(dataset_dir, \"training\")\n",
        "with open(os.path.join(dataset_dir,\"train_gt.json\")) as f:\n",
        "  dic = json.load(f)\n",
        "  dataframe = pd.DataFrame(dic.items())\n",
        "  dataframe.rename(columns = {0:'filename', 1:'class'}, inplace = True)\n",
        "  dataframe = dataframe.sample(frac=1, random_state=SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSx96bmu3riQ"
      },
      "source": [
        "#auxiliary variable to handle Colab's error due to GPU's exhaustion or random runtime disconnections\n",
        "stopped_train = False\n",
        "stopped_train_ft = False\n",
        "\n",
        "kf = KFold(n_splits = 5, shuffle = True, random_state = SEED)\n",
        "model_number = 0\n",
        "skip_first = -1\n",
        "for result in kf.split(dataframe):\n",
        "  if model_number > skip_first:\n",
        "    train = dataframe.iloc[result[0]]\n",
        "    valid = dataframe.iloc[result[1]]\n",
        "    train[\"class\"] = train[\"class\"].astype('string')\n",
        "    valid[\"class\"] = valid[\"class\"].astype('string')\n",
        "    \n",
        "    train_gen = train_data_gen.flow_from_dataframe(train,\n",
        "                                                training_dir,\n",
        "                                                batch_size=BS,\n",
        "                                                target_size=(IMG_H, IMG_W),\n",
        "                                                class_mode='categorical',\n",
        "                                                shuffle=True,\n",
        "                                                seed=SEED)\n",
        "    \n",
        "    validation_gen = valid_data_gen.flow_from_dataframe(valid,\n",
        "                                                training_dir,\n",
        "                                                batch_size=BS,\n",
        "                                                target_size=(IMG_H, IMG_W),\n",
        "                                                class_mode='categorical',\n",
        "                                                shuffle=True,\n",
        "                                                seed=SEED)\n",
        "    #create the dataset objects\n",
        "    train_ds = tf.data.Dataset.from_generator(lambda: train_gen, \n",
        "                                              output_types=(tf.float32, tf.float32),\n",
        "                                              output_shapes= ([None, IMG_H, IMG_W, 3], [None, 3]))\n",
        "\n",
        "    train_ds = train_ds.repeat()\n",
        "\n",
        "    valid_ds = tf.data.Dataset.from_generator(lambda: validation_gen,\n",
        "                                              output_types = (tf.float32, tf.float32),\n",
        "                                              output_shapes = ([None, IMG_H, IMG_W, 3], [None, 3]))\n",
        "    valid_ds = valid_ds.repeat()\n",
        "\n",
        "    name = 'VGG_512+448_' + str(model_number)\n",
        "\n",
        "    base_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=(IMG_H,IMG_W,3))\n",
        "    base_model.trainable = False\n",
        "        \n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.1))\n",
        "    model.add(tf.keras.layers.Dense(units=448, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.1))\n",
        "    model.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n",
        "    model.compile(optimizer=get_optimizer(lr_schedule=1e-4),\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    if stopped_train and model_number == (skip_first+1):\n",
        "      model_to_load = 'VGG_512+448_' + str(model_number)\n",
        "      ckpt_path = os.path.join(drive_root_folder,'classification_result',model_to_load, 'ckpts','cp.ckpt')\n",
        "      model.load_weights(ckpt_path)\n",
        "\n",
        "    print('\\nTrain' + str(model_number) + '\\n')\n",
        "    model.fit(x=train_ds,\n",
        "              epochs=30, \n",
        "              steps_per_epoch = len(train_gen),\n",
        "              validation_data = valid_ds,\n",
        "              validation_steps = len(validation_gen), \n",
        "              callbacks = get_callbacks(name),\n",
        "              verbose = 1)\n",
        "    \n",
        "    model_to_load = 'VGG_512+448_' + str(model_number)\n",
        "    ckpt_path = os.path.join(drive_root_folder,'classification_result',model_to_load, 'ckpts','cp.ckpt')\n",
        "    model.load_weights(ckpt_path)\n",
        "    base_model.trainable = True\n",
        "\n",
        "    if stopped_train_ft and model_number == (skip_first+1):\n",
        "      model_to_load = 'FT-VGG_512+448_' + str(model_number)\n",
        "      ckpt_path = os.path.join(drive_root_folder,'classification_result',model_to_load, 'ckpts','cp.ckpt')\n",
        "      model.load_weights(ckpt_path)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-5),  # Low learning rate\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    print('\\nFine Tuning\\n')\n",
        "    epochs = 60\n",
        "    model.fit(train_ds,\n",
        "              epochs = epochs,\n",
        "              validation_data = valid_ds,\n",
        "              steps_per_epoch = len(train_gen),\n",
        "              validation_steps = len(validation_gen),\n",
        "              callbacks=get_callbacks('FT-VGG_512+448_' + str(model_number), es = True, patience=12))\n",
        "    \n",
        "  model_number += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi5B6RK5b52M"
      },
      "source": [
        "## Predictions with majority voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H10x3cXhY0Kd"
      },
      "source": [
        "First load all the k models trained before: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WheqqB9OBBXV"
      },
      "source": [
        "def remodel():\n",
        "  base_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=(IMG_H,IMG_W,3))\n",
        "  base_model.trainable = False\n",
        "      \n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(base_model)\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "  model.add(tf.keras.layers.Dropout(0.1))\n",
        "  model.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XkOBdf5pF9x"
      },
      "source": [
        "def load_all_models(n_models=5):\n",
        "  all_models = list()\n",
        "  for n in range(n_models):\n",
        "    model = remodel()\n",
        "    model_to_load = 'FT-VGG16_' + str(n)\n",
        "    ckpt_path = os.path.join(drive_root_folder,'classification_result',model_to_load, 'ckpts','cp.ckpt')\n",
        "    model.load_weights(ckpt_path)\n",
        "    all_models.append(model)\n",
        "    print('>loaded %s' %ckpt_path)\n",
        "  return all_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toULhRqLY_UF"
      },
      "source": [
        "Predictions are performed taking care of all the output probabilities of the k models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkYcnfXDpVS6"
      },
      "source": [
        "def ensemble_predictions(members, testX):\n",
        "  # make predictions\n",
        "  yhats = [model.predict(testX) for model in members]\n",
        "  yhats = np.array(yhats)\n",
        "  # sum across ensemble members\n",
        "  summed = np.sum(yhats, axis=0)\n",
        "  # argmax across classes\n",
        "  result = np.argmax(summed, axis=1)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWNWkCg5cjsh"
      },
      "source": [
        "Then load the test images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gif_tFw0qTD1"
      },
      "source": [
        "import pathlib\n",
        "path = os.path.join(os.getcwd(),'MaskDataset', 'test')\n",
        "data_dir = pathlib.Path(path)\n",
        "\n",
        "test_image_filenames = list(data_dir.glob('*.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOTq1FWHqWgY"
      },
      "source": [
        "test_normalization = tf.keras.Sequential([\n",
        "                                       tf.keras.layers.experimental.preprocessing.Rescaling(1./255)                 \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hqH6NVsZUhB"
      },
      "source": [
        "For each image the output class is the best accordin to all the five models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4yp9cWapIrM"
      },
      "source": [
        "def make_prediction(all_models):\n",
        "  results = {}\n",
        "  results_expanded = {}\n",
        "  for test_image in test_image_filenames:\n",
        "    img = tf.keras.preprocessing.image.load_img(test_image, target_size=(IMG_H, IMG_W))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "    normalized_img = test_normalization(img_array, training=True)\n",
        "\n",
        "    result = ensemble_predictions(all_models, normalized_img)\n",
        "    results_expanded[os.path.basename(test_image)]=result\n",
        "    results[os.path.basename(test_image)]=result[0]\n",
        "  return results, results_expanded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afnjAGyQx0l3"
      },
      "source": [
        "def create_csv(results, results_dir):\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "        f.write('Id,Category\\n')\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "700fas07x3mY"
      },
      "source": [
        "create_csv(make_prediction(load_all_models())[0], drive_root_folder)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}